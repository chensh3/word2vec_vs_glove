{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_models_on_synonyms.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install glove-python-binary\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKh_GPnDxykG",
        "outputId": "1e00ec76-4017-466a-e039-d90eed70aac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting glove-python-binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.21.6)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Glove,Corpus\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class GV:\n",
        "    def __init__(self,vector_size, window_size, lr = 0.05, workers = 4,data=[]):\n",
        "        self.vector_size = vector_size\n",
        "        self.window_size = window_size\n",
        "        self.learning_rate = lr\n",
        "        self.workers = workers\n",
        "\n",
        "        self.data = data\n",
        "        self.model = None\n",
        "        self.vocab = None\n",
        "        self.corpus=Corpus()\n",
        "        if data!=[]:\n",
        "            self.corpus.fit(self.data, window=self.window_size)\n",
        "  \n",
        "    def load_glove(self,path):\n",
        "        self.model=Glove.load(path)\n",
        "        self.vocab=list(self.model.dictionary.keys())\n",
        "\n",
        "    def train_glove(self,num_epochs,model_path=\"\"):\n",
        "        # self.corpus.fit(self.data, window=self.window_size)\n",
        "        glove = Glove(no_components=self.vector_size, learning_rate=self.learning_rate) \n",
        "        start = time.perf_counter()\n",
        "        glove.fit(self.corpus.matrix, epochs=num_epochs, no_threads=self.workers, verbose=True)\n",
        "        end = time.perf_counter()\n",
        "        glove.add_dictionary(self.corpus.dictionary)\n",
        "        self.model=glove\n",
        "        self.vocab=list(self.model.dictionary.keys())\n",
        "        if model_path != \"\":\n",
        "            self.model.save(model_path)\n",
        "        return end-start\n",
        "    def get_top_similar_words(self, word, num = 5):\n",
        "        most_similar = np.array(self.model.most_similar(word, number = num+1)).transpose()[0]\n",
        "        return most_similar\n",
        "\n",
        "\n",
        "    def check_synonyms_in_model(self,target, words, num):\n",
        "        count = 0\n",
        "        similar_words = self.get_top_similar_words(target, num)\n",
        "        for word in words:\n",
        "            if word in similar_words:\n",
        "                count += 1\n",
        "        return count\n",
        "\n",
        "\n",
        "\n",
        "class W2V:\n",
        "    def __init__(self, vector_size, window_size, sg = 1, lr = 0.05, workers = 4,data=[]):\n",
        "        self.vector_size = vector_size\n",
        "        self.window_size = window_size\n",
        "        self.sg = sg\n",
        "        self.learning_rate = lr\n",
        "        self.workers = workers\n",
        "\n",
        "        self.data = data\n",
        "        self.model = None\n",
        "        self.vocab = None\n",
        "\n",
        "    def train_word2vec(self, num_epochs, model_path = \"\"):\n",
        "        print(\"\\n\\nTraining word2vec model\")\n",
        "        # Create Skip Gram model ( sg=1 )  or CBOW ( sg=0 )\n",
        "        start = time.perf_counter()\n",
        "\n",
        "        word2vec = Word2Vec(sentences=self.data,size=self.vector_size,\n",
        "                            window=self.window_size,sg=self.sg,min_count =0,\n",
        "                            # epochs=num_epochs,\n",
        "                            workers=self.workers,alpha=self.learning_rate)\n",
        "                            \n",
        "            # self.data, self.vector_size,self.window_size,\n",
        "            #                 self.sg,num_epochs,self.workers, self.learning_rate)\n",
        "\n",
        "        end = time.perf_counter()\n",
        "\n",
        "        print(\"Finished training word2vec model\\n\")\n",
        "\n",
        "        self.model = word2vec\n",
        "        self.vocab = self.model.wv.index2word\n",
        "        # self.vocab = self.model.wv.index_to_key\n",
        "        if model_path != \"\":\n",
        "            self.model.save(model_path)\n",
        "\n",
        "        return end - start\n",
        "\n",
        "    def check_one_word(self, word):\n",
        "        a = []\n",
        "        for i in self.vocab:\n",
        "            a.append(self.model.wv.similarity(word, i))\n",
        "        return a\n",
        "\n",
        "  \n",
        "    def delete_unknown_words(self, words):\n",
        "\n",
        "        known_words = [word for word in words if word in self.vocab]\n",
        "        unknown_words = list(set(words) - set(known_words))\n",
        "        return known_words, unknown_words\n",
        "\n",
        "    def get_top_similar_words(self, word, num = 5):\n",
        "        most_similar = np.array(self.model.wv.most_similar(word, topn = num)).transpose()[0]\n",
        "        return most_similar\n",
        "\n",
        "\n",
        "    def check_synonyms_in_model(self,target, words, num):\n",
        "        count = 0\n",
        "        similar_words = self.get_top_similar_words(target, num)\n",
        "        for word in words:\n",
        "            if word in similar_words:\n",
        "                count += 1\n",
        "        return count\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model = Word2Vec.load(path)\n",
        "        self.vocab = self.model.wv.index2word\n",
        "        # self.vocab = self.model.wv.index_to_key"
      ],
      "metadata": {
        "id": "dX-E-0Pa4g92"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ZTOhipF-xq-R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "# from nlp_models.py import *\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# from patent_dataset import df\n",
        "\n",
        "# full_text = df.full_text\n",
        "\n",
        "\n",
        "def delete_words_not_in_vocab(data, col, model1):\n",
        "    index = [i for i, target in enumerate(data[col]) if target in model1.vocab]\n",
        "    filtered_data = data.iloc[index]\n",
        "    return filtered_data\n",
        "\n",
        "\n",
        "# path = r'C:\\Users\\Chen\\Desktop\\Masters_degree\\NLP'  # PC Chen\n",
        "path = r'C:/Users/Chen/Documents/Masters_degree/word2vec_vs_glove'  # Laptop Chen\n",
        "# full_text = pd.read_pickle(r\"C:\\Users\\Chen\\Desktop\\Masters_degree\\NLP\\full_text.pkl\")\n",
        "\n",
        "# model = W2V(300, 2, 1, 0.05, 4)\n",
        "# model.prepare_data(full_text, limit_data=100)\n",
        "# train_time = model.train_word2vec(num_epochs=30, model_path=path + r'/output/word2vec_model')\n",
        "\n",
        "# print(f\"training the model took : {train_time:0.4f} sec \\n\")\n",
        "\n",
        "glove_model = GV(300, 2, 0.05, 4)\n",
        "glove_model.load_glove(path='/content/drive/MyDrive/Colab Notebooks/output/glove_model')\n",
        "\n",
        "w2v_model = W2V(300, 2, 1, 0.05, 4)\n",
        "w2v_model.load_model('/content/drive/MyDrive/Colab Notebooks/output/word2vec_model')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/synonyms.csv').dropna()\n",
        "df = delete_words_not_in_vocab(df, \"lemma\", w2v_model)\n",
        "df = df.drop(\"part_of_speech\", axis=1)\n",
        "df.synonyms = [i.replace(\"|\", \";\").split(\";\") for i in df.synonyms]\n",
        "\n",
        "df = df.dropna().reset_index().drop(\"index\", axis=1)\n",
        "data = df.groupby(\"lemma\").sum().reset_index().dropna()\n",
        "data = delete_words_not_in_vocab(data, \"lemma\", w2v_model)\n"
      ],
      "metadata": {
        "id": "HVlQP55f2hT1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\")\n",
        "count_w2v = []\n",
        "count_glove = []\n",
        "for i in range(len(data)):\n",
        "    count_w2v.append(w2v_model.check_synonyms_in_model(data.loc[i, \"lemma\"], data.loc[i, \"synonyms\"], 5))\n",
        "    count_glove.append(glove_model.check_synonyms_in_model(data.loc[i, \"lemma\"], data.loc[i, \"synonyms\"], 5))\n",
        "    print(\"\\r\", end='')\n",
        "    print(f\"{i}/7283 number of matches in: w2v:{sum(count_w2v)}\",f\" glove:{sum(count_glove)}\",end='', flush=True)\n",
        "\n",
        "    # print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI1hbiHj5R_s",
        "outputId": "da94f091-9d48-4a56-e490-3f8801d082a3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "7282/7283 number of matches in: w2v:409  glove:273"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7u1yZe4E-USH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}