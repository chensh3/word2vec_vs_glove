{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install glove-python-binary\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE5QHuiNdu1C",
        "outputId": "32ee9612-885c-4ca3-d56a-b7b1b3561912"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: glove-python-binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.21.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "JSp0Ui18cjq6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import time\n",
        "from glove import Glove,Corpus\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def data_preprocessing(raw_file):\n",
        "    data = []\n",
        "    # iterate through each sentence in the file\n",
        "    for i in sent_tokenize(raw_file):\n",
        "        temp = []\n",
        "        # tokenize the sentence into words and avoid stop words\n",
        "        for j in word_tokenize(i):\n",
        "            if not j.isalpha():\n",
        "                continue\n",
        "\n",
        "            temp.append(j.lower())\n",
        "        data.append(temp)\n",
        "\n",
        "    flat_list = list(set(np.concatenate(data).flat))\n",
        "    return data, flat_list\n",
        "\n",
        "def prepare_data( data, limit_data = None):\n",
        "    db = []\n",
        "    words = []\n",
        "    for i, patent in enumerate(data[:limit_data]):\n",
        "        print(f\"training on patent num: {i}\", end = '\\r')\n",
        "        f = patent.replace(\"\\n\", \" \")\n",
        "        db_file, words_file = data_preprocessing(f)\n",
        "        db = db + db_file\n",
        "        words = words + words_file\n",
        "\n",
        "    flat_list = list(set(words))\n",
        "\n",
        "    return db,flat_list\n",
        "\n",
        "\n",
        "path = r'C:\\Users\\Chen\\Desktop\\Masters_degree\\NLP'  # PC Chen\n",
        "# path = r'C:/Users/Chen/Documents/Masters_degree/word2vec_vs_glove' # Laptop Chen\n",
        "class W2V:\n",
        "    def __init__(self, vector_size, window_size, sg = 1, lr = 0.05, workers = 4,data=[]):\n",
        "        self.vector_size = vector_size\n",
        "        self.window_size = window_size\n",
        "        self.sg = sg\n",
        "        self.learning_rate = lr\n",
        "        self.workers = workers\n",
        "\n",
        "        self.data = data\n",
        "        self.model = None\n",
        "        self.vocab = None\n",
        "\n",
        "    def train_word2vec(self, num_epochs, model_path = \"\"):\n",
        "        print(\"\\n\\nTraining word2vec model\")\n",
        "        # Create Skip Gram model ( sg=1 )  or CBOW ( sg=0 )\n",
        "        start = time.perf_counter()\n",
        "\n",
        "        word2vec = Word2Vec(sentences=self.data,size=self.vector_size,\n",
        "                            window=self.window_size,sg=self.sg,min_count =0,\n",
        "                            # epochs=num_epochs,\n",
        "                            workers=self.workers,alpha=self.learning_rate)\n",
        "                            \n",
        "            # self.data, self.vector_size,self.window_size,\n",
        "            #                 self.sg,num_epochs,self.workers, self.learning_rate)\n",
        "\n",
        "        end = time.perf_counter()\n",
        "\n",
        "        print(\"Finished training word2vec model\\n\")\n",
        "\n",
        "        self.model = word2vec\n",
        "        self.vocab = self.model.wv.index2word\n",
        "        # self.vocab = self.model.wv.index_to_key\n",
        "        if model_path != \"\":\n",
        "            self.model.save(model_path)\n",
        "\n",
        "        return end - start\n",
        "\n",
        "    def check_one_word(self, word):\n",
        "        a = []\n",
        "        for i in self.vocab:\n",
        "            a.append(self.model.wv.similarity(word, i))\n",
        "        return a\n",
        "\n",
        "  \n",
        "    def delete_unknown_words(self, words):\n",
        "\n",
        "        known_words = [word for word in words if word in self.vocab]\n",
        "        unknown_words = list(set(words) - set(known_words))\n",
        "        return known_words, unknown_words\n",
        "\n",
        "    def get_top_similar_words(self, word, num = 5):\n",
        "        most_similar = np.array(self.model.wv.most_similar(word, topn = num)).transpose()[0]\n",
        "        return most_similar\n",
        "\n",
        "\n",
        "    def check_synonyms_in_model(self,target, words, num):\n",
        "        count = 0\n",
        "        similar_words = self.get_top_similar_words(target, num)\n",
        "        for word in words:\n",
        "            if word in similar_words:\n",
        "                count += 1\n",
        "        return count\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model = Word2Vec.load(path)\n",
        "        self.vocab = self.model.wv.index2word\n",
        "        # self.vocab = self.model.wv.index_to_key\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path = r'C:\\Users\\Chen\\Desktop\\Masters_degree\\NLP'  # PC Chen\n",
        "class GV:\n",
        "    def __init__(self,vector_size, window_size, lr = 0.05, workers = 4,data=[]):\n",
        "        self.vector_size = vector_size\n",
        "        self.window_size = window_size\n",
        "        self.learning_rate = lr\n",
        "        self.workers = workers\n",
        "\n",
        "        self.data = data\n",
        "        self.model = None\n",
        "        self.vocab = None\n",
        "        self.corpus=Corpus()\n",
        "        if data!=[]:\n",
        "            self.corpus.fit(self.data, window=self.window_size)\n",
        "  \n",
        "    def load_glove(self,path):\n",
        "        self.model=Glove.load(path)\n",
        "        self.vocab=self.model.dictionary.keys()\n",
        "\n",
        "    def train_glove(self,num_epochs,model_path=\"\"):\n",
        "        # self.corpus.fit(self.data, window=self.window_size)\n",
        "        glove = Glove(no_components=self.vector_size, learning_rate=self.learning_rate) \n",
        "        start = time.perf_counter()\n",
        "        glove.fit(self.corpus.matrix, epochs=num_epochs, no_threads=self.workers, verbose=True)\n",
        "        end = time.perf_counter()\n",
        "        glove.add_dictionary(self.corpus.dictionary)\n",
        "        self.model=glove\n",
        "        self.vocab=self.model.dictionary.keys()\n",
        "        if model_path != \"\":\n",
        "            self.model.save(model_path)\n",
        "        return end-start\n",
        "    def get_top_similar_words(self, word, num = 5):\n",
        "        most_similar = np.array(self.model.most_similar(word, number = num+1)).transpose()[0]\n",
        "        return most_similar\n",
        "\n",
        "\n",
        "    def check_synonyms_in_model(self,target, words, num):\n",
        "        count = 0\n",
        "        similar_words = self.get_top_similar_words(target, num)\n",
        "        for word in words:\n",
        "            if word in similar_words:\n",
        "                count += 1\n",
        "        return count\n",
        "\n",
        "\n",
        "# path = r'C:/Users/Chen/Documents/Masters_degree/word2vec_vs_glove' # Laptop Chen"
      ],
      "metadata": {
        "id": "wnx-MKALjDNe"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text=pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/full_text.pkl')\n",
        "db,a=prepare_data(full_text, limit_data = 100)\n",
        "glove_model = GV(300, 2, 0.05, 4,data=db)\n",
        "train_time = glove_model.train_glove(num_epochs = 30, model_path = '/content/drive/MyDrive/Colab Notebooks/output/glove_model')\n",
        "\n",
        "print(f\"training the GloVe model took : {train_time:0.4f} sec \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "P7Np7lITjJGs",
        "outputId": "cb5ff4dc-abaf-4b19-f947-cde530b69699"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-39374b13bda0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfull_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/full_text.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# glove_model = GV(300, 2, 0.05, 4,data=db)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train_time = glove_model.train_glove(num_epochs = 30, model_path = '/content/drive/MyDrive/Colab Notebooks/output/glove_model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# glove_model = GV(300, 2, 0.05, 4)\n",
        "# glove_model.load_glove(path='/content/drive/MyDrive/Colab Notebooks/output/glove_model')"
      ],
      "metadata": {
        "id": "Un2ajjLxygGY"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "w2v_model = W2V(300, 2, 1, 0.05, 4,data=db)\n",
        "train_time = w2v_model.train_word2vec(num_epochs = 30, model_path = '/content/drive/MyDrive/Colab Notebooks/output/word2vec_model')\n",
        "\n",
        "print(f\"training the model took : {train_time:0.4f} sec \\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSguQElVd7PU",
        "outputId": "4940194a-f911-41d7-f567-b0d08e4d6357"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Training word2vec model\n",
            "Finished training word2vec model\n",
            "\n",
            "training the model took : 23.0080 sec \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hPjAUNJP8d1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}